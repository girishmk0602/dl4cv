With BATCH NORMALISATION
========================
(dl4cv) (master) deep $ python minvggnet_cifar10.py -o vgg_cifar10.png
Using TensorFlow backend.
[INFO] loading CIFAR-10 data...
[INFO] compiling model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
activation_4 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2097664   
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 512)               2048      
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_6 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,171,178
Trainable params: 2,169,770
Non-trainable params: 1,408
_________________________________________________________________
[INFO] training network...
Train on 50000 samples, validate on 10000 samples
Epoch 1/40
2017-11-20 20:55:42.770315: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-20 20:55:42.770343: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-11-20 20:55:42.770348: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-20 20:55:42.770352: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
50000/50000 [==============================] - 632s - loss: 1.6230 - acc: 0.4587 - val_loss: 1.4771 - val_acc: 0.4929
Epoch 2/40
50000/50000 [==============================] - 591s - loss: 1.1625 - acc: 0.5933 - val_loss: 0.9801 - val_acc: 0.6525
Epoch 3/40
50000/50000 [==============================] - 571s - loss: 0.9785 - acc: 0.6586 - val_loss: 0.9086 - val_acc: 0.6774
Epoch 4/40
50000/50000 [==============================] - 569s - loss: 0.8715 - acc: 0.6954 - val_loss: 0.7926 - val_acc: 0.7176
Epoch 5/40
50000/50000 [==============================] - 570s - loss: 0.7931 - acc: 0.7226 - val_loss: 0.7042 - val_acc: 0.7542
Epoch 6/40
50000/50000 [==============================] - 571s - loss: 0.7325 - acc: 0.7428 - val_loss: 0.8055 - val_acc: 0.7279
Epoch 7/40
50000/50000 [==============================] - 569s - loss: 0.6907 - acc: 0.7560 - val_loss: 0.6758 - val_acc: 0.7669
Epoch 8/40
50000/50000 [==============================] - 569s - loss: 0.6501 - acc: 0.7737 - val_loss: 0.7054 - val_acc: 0.7613
Epoch 9/40
50000/50000 [==============================] - 568s - loss: 0.6117 - acc: 0.7848 - val_loss: 0.6200 - val_acc: 0.7829
Epoch 10/40
50000/50000 [==============================] - 569s - loss: 0.5811 - acc: 0.7921 - val_loss: 0.6173 - val_acc: 0.7886
Epoch 11/40
50000/50000 [==============================] - 569s - loss: 0.5562 - acc: 0.8028 - val_loss: 0.6478 - val_acc: 0.7771
Epoch 12/40
50000/50000 [==============================] - 569s - loss: 0.5358 - acc: 0.8093 - val_loss: 0.5834 - val_acc: 0.7994
Epoch 13/40
50000/50000 [==============================] - 569s - loss: 0.5162 - acc: 0.8185 - val_loss: 0.5811 - val_acc: 0.7993
Epoch 14/40
50000/50000 [==============================] - 569s - loss: 0.4898 - acc: 0.8265 - val_loss: 0.5794 - val_acc: 0.8009
Epoch 15/40
50000/50000 [==============================] - 568s - loss: 0.4691 - acc: 0.8338 - val_loss: 0.5704 - val_acc: 0.8059
Epoch 16/40
50000/50000 [==============================] - 569s - loss: 0.4597 - acc: 0.8369 - val_loss: 0.5894 - val_acc: 0.8001
Epoch 17/40
50000/50000 [==============================] - 569s - loss: 0.4461 - acc: 0.8412 - val_loss: 0.5534 - val_acc: 0.8090
Epoch 18/40
50000/50000 [==============================] - 569s - loss: 0.4274 - acc: 0.8481 - val_loss: 0.5557 - val_acc: 0.8133
Epoch 19/40
50000/50000 [==============================] - 569s - loss: 0.4130 - acc: 0.8548 - val_loss: 0.5590 - val_acc: 0.8132
Epoch 20/40
50000/50000 [==============================] - 570s - loss: 0.4036 - acc: 0.8560 - val_loss: 0.5595 - val_acc: 0.8124
Epoch 21/40
50000/50000 [==============================] - 569s - loss: 0.3883 - acc: 0.8628 - val_loss: 0.5449 - val_acc: 0.8176
Epoch 22/40
50000/50000 [==============================] - 568s - loss: 0.3765 - acc: 0.8660 - val_loss: 0.5640 - val_acc: 0.8113
Epoch 23/40
50000/50000 [==============================] - 569s - loss: 0.3691 - acc: 0.8696 - val_loss: 0.5450 - val_acc: 0.8204
Epoch 24/40
50000/50000 [==============================] - 570s - loss: 0.3534 - acc: 0.8738 - val_loss: 0.5476 - val_acc: 0.8195
Epoch 25/40
50000/50000 [==============================] - 569s - loss: 0.3465 - acc: 0.8762 - val_loss: 0.5359 - val_acc: 0.8227
Epoch 26/40
50000/50000 [==============================] - 573s - loss: 0.3404 - acc: 0.8795 - val_loss: 0.5496 - val_acc: 0.8194
Epoch 27/40
50000/50000 [==============================] - 568s - loss: 0.3329 - acc: 0.8798 - val_loss: 0.5372 - val_acc: 0.8223
Epoch 28/40
50000/50000 [==============================] - 568s - loss: 0.3255 - acc: 0.8835 - val_loss: 0.5624 - val_acc: 0.8165
Epoch 29/40
50000/50000 [==============================] - 569s - loss: 0.3160 - acc: 0.8884 - val_loss: 0.5503 - val_acc: 0.8211
Epoch 30/40
50000/50000 [==============================] - 569s - loss: 0.3061 - acc: 0.8920 - val_loss: 0.5452 - val_acc: 0.8228
Epoch 31/40
50000/50000 [==============================] - 568s - loss: 0.3015 - acc: 0.8924 - val_loss: 0.5473 - val_acc: 0.8237
Epoch 32/40
50000/50000 [==============================] - 569s - loss: 0.2967 - acc: 0.8934 - val_loss: 0.5509 - val_acc: 0.8245
Epoch 33/40
50000/50000 [==============================] - 568s - loss: 0.2854 - acc: 0.8981 - val_loss: 0.5423 - val_acc: 0.8218
Epoch 34/40
50000/50000 [==============================] - 569s - loss: 0.2842 - acc: 0.8981 - val_loss: 0.5611 - val_acc: 0.8173
Epoch 35/40
50000/50000 [==============================] - 569s - loss: 0.2736 - acc: 0.9025 - val_loss: 0.5606 - val_acc: 0.8212
Epoch 36/40
50000/50000 [==============================] - 569s - loss: 0.2764 - acc: 0.9005 - val_loss: 0.5396 - val_acc: 0.8271
Epoch 37/40
50000/50000 [==============================] - 569s - loss: 0.2653 - acc: 0.9053 - val_loss: 0.5463 - val_acc: 0.8259
Epoch 38/40
50000/50000 [==============================] - 570s - loss: 0.2570 - acc: 0.9077 - val_loss: 0.5421 - val_acc: 0.8264
Epoch 39/40
50000/50000 [==============================] - 570s - loss: 0.2531 - acc: 0.9086 - val_loss: 0.5538 - val_acc: 0.8233
Epoch 40/40
50000/50000 [==============================] - 569s - loss: 0.2512 - acc: 0.9103 - val_loss: 0.5545 - val_acc: 0.8257
[INFO] evluating network...
             precision    recall  f1-score   support

   airplane       0.85      0.82      0.84      1000
 automobile       0.93      0.92      0.92      1000
       bird       0.77      0.72      0.75      1000
        cat       0.69      0.65      0.67      1000
       deer       0.77      0.83      0.80      1000
        dog       0.75      0.72      0.74      1000
       frog       0.84      0.90      0.87      1000
      horse       0.85      0.88      0.87      1000
       ship       0.91      0.91      0.91      1000
      truck       0.88      0.90      0.89      1000

avg / total       0.82      0.83      0.82     10000

===========================
WITHOUT BATCH NORMALISATION
===========================

(dl4cv) (master) deep $ python minvggnet_cifar10.py -o vgg_cifar10.png -b no
Using TensorFlow backend.
[INFO] loading CIFAR-10 data...
[INFO] compiling model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
activation_4 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2097664   
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_6 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,168,362
Trainable params: 2,168,362
Non-trainable params: 0
_________________________________________________________________
[INFO] training network...
Train on 50000 samples, validate on 10000 samples
Epoch 1/40
2017-11-21 09:59:21.195067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 09:59:21.195098: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 09:59:21.195106: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 09:59:21.195112: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
50000/50000 [==============================] - 373s - loss: 1.8131 - acc: 0.3339 - val_loss: 1.4685 - val_acc: 0.4687
Epoch 2/40
50000/50000 [==============================] - 372s - loss: 1.3869 - acc: 0.4953 - val_loss: 1.2450 - val_acc: 0.5507
Epoch 3/40
50000/50000 [==============================] - 367s - loss: 1.2036 - acc: 0.5681 - val_loss: 1.0613 - val_acc: 0.6269
Epoch 4/40
50000/50000 [==============================] - 371s - loss: 1.0703 - acc: 0.6172 - val_loss: 0.9849 - val_acc: 0.6514
Epoch 5/40
50000/50000 [==============================] - 367s - loss: 0.9727 - acc: 0.6548 - val_loss: 0.9404 - val_acc: 0.6731
Epoch 6/40
50000/50000 [==============================] - 396s - loss: 0.9014 - acc: 0.6825 - val_loss: 0.8483 - val_acc: 0.7007
Epoch 7/40
50000/50000 [==============================] - 381s - loss: 0.8401 - acc: 0.7032 - val_loss: 0.7831 - val_acc: 0.7282
Epoch 8/40
50000/50000 [==============================] - 398s - loss: 0.7864 - acc: 0.7238 - val_loss: 0.7584 - val_acc: 0.7313
Epoch 9/40
50000/50000 [==============================] - 376s - loss: 0.7444 - acc: 0.7364 - val_loss: 0.7319 - val_acc: 0.7412
Epoch 10/40
50000/50000 [==============================] - 378s - loss: 0.7052 - acc: 0.7510 - val_loss: 0.7017 - val_acc: 0.7512
Epoch 11/40
50000/50000 [==============================] - 398s - loss: 0.6686 - acc: 0.7621 - val_loss: 0.6962 - val_acc: 0.7538
Epoch 12/40
50000/50000 [==============================] - 379s - loss: 0.6428 - acc: 0.7734 - val_loss: 0.6849 - val_acc: 0.7601
Epoch 13/40
50000/50000 [==============================] - 387s - loss: 0.6120 - acc: 0.7838 - val_loss: 0.6731 - val_acc: 0.7656
Epoch 14/40
50000/50000 [==============================] - 400s - loss: 0.5859 - acc: 0.7944 - val_loss: 0.6482 - val_acc: 0.7756
Epoch 15/40
50000/50000 [==============================] - 379s - loss: 0.5652 - acc: 0.7989 - val_loss: 0.6356 - val_acc: 0.7774
Epoch 16/40
50000/50000 [==============================] - 356s - loss: 0.5415 - acc: 0.8094 - val_loss: 0.6222 - val_acc: 0.7845
Epoch 17/40
50000/50000 [==============================] - 371s - loss: 0.5240 - acc: 0.8123 - val_loss: 0.6430 - val_acc: 0.7778
Epoch 18/40
50000/50000 [==============================] - 364s - loss: 0.5030 - acc: 0.8202 - val_loss: 0.6258 - val_acc: 0.7841
Epoch 19/40
50000/50000 [==============================] - 354s - loss: 0.4858 - acc: 0.8265 - val_loss: 0.6358 - val_acc: 0.7835
Epoch 20/40
50000/50000 [==============================] - 352s - loss: 0.4715 - acc: 0.8329 - val_loss: 0.6277 - val_acc: 0.7880
Epoch 21/40
50000/50000 [==============================] - 357s - loss: 0.4570 - acc: 0.8369 - val_loss: 0.6091 - val_acc: 0.7957
Epoch 22/40
50000/50000 [==============================] - 370s - loss: 0.4394 - acc: 0.8432 - val_loss: 0.6119 - val_acc: 0.7953
Epoch 23/40
50000/50000 [==============================] - 357s - loss: 0.4239 - acc: 0.8504 - val_loss: 0.6075 - val_acc: 0.7952
Epoch 24/40
50000/50000 [==============================] - 380s - loss: 0.4134 - acc: 0.8521 - val_loss: 0.6161 - val_acc: 0.7970
Epoch 25/40
50000/50000 [==============================] - 376s - loss: 0.3955 - acc: 0.8588 - val_loss: 0.6190 - val_acc: 0.7959
Epoch 26/40
50000/50000 [==============================] - 373s - loss: 0.3836 - acc: 0.8607 - val_loss: 0.6081 - val_acc: 0.7971
Epoch 27/40
50000/50000 [==============================] - 361s - loss: 0.3749 - acc: 0.8663 - val_loss: 0.6234 - val_acc: 0.7953
Epoch 28/40
50000/50000 [==============================] - 355s - loss: 0.3616 - acc: 0.8705 - val_loss: 0.6140 - val_acc: 0.7999
Epoch 29/40
50000/50000 [==============================] - 344s - loss: 0.3589 - acc: 0.8705 - val_loss: 0.6133 - val_acc: 0.7999
Epoch 30/40
50000/50000 [==============================] - 380s - loss: 0.3455 - acc: 0.8768 - val_loss: 0.6167 - val_acc: 0.7967
Epoch 31/40
50000/50000 [==============================] - 355s - loss: 0.3377 - acc: 0.8770 - val_loss: 0.6288 - val_acc: 0.7978
Epoch 32/40
50000/50000 [==============================] - 347s - loss: 0.3326 - acc: 0.8812 - val_loss: 0.6074 - val_acc: 0.8032
Epoch 33/40
50000/50000 [==============================] - 342s - loss: 0.3190 - acc: 0.8868 - val_loss: 0.6199 - val_acc: 0.8017
Epoch 34/40
50000/50000 [==============================] - 341s - loss: 0.3136 - acc: 0.8869 - val_loss: 0.6172 - val_acc: 0.8010
Epoch 35/40
50000/50000 [==============================] - 341s - loss: 0.3031 - acc: 0.8912 - val_loss: 0.6198 - val_acc: 0.8015
Epoch 36/40
50000/50000 [==============================] - 341s - loss: 0.3043 - acc: 0.8897 - val_loss: 0.6069 - val_acc: 0.8018
Epoch 37/40
50000/50000 [==============================] - 341s - loss: 0.2879 - acc: 0.8969 - val_loss: 0.6230 - val_acc: 0.8018
Epoch 38/40
50000/50000 [==============================] - 341s - loss: 0.2846 - acc: 0.8980 - val_loss: 0.6228 - val_acc: 0.8048
Epoch 39/40
50000/50000 [==============================] - 341s - loss: 0.2728 - acc: 0.9020 - val_loss: 0.6209 - val_acc: 0.8047
Epoch 40/40
50000/50000 [==============================] - 341s - loss: 0.2725 - acc: 0.9021 - val_loss: 0.6213 - val_acc: 0.8026
[INFO] evluating network...
             precision    recall  f1-score   support

   airplane       0.82      0.85      0.83      1000
 automobile       0.88      0.92      0.90      1000
       bird       0.76      0.70      0.73      1000
        cat       0.64      0.60      0.62      1000
       deer       0.76      0.77      0.77      1000
        dog       0.69      0.74      0.71      1000
       frog       0.85      0.86      0.86      1000
      horse       0.86      0.84      0.85      1000
       ship       0.90      0.88      0.89      1000
      truck       0.87      0.86      0.86      1000

avg / total       0.80      0.80      0.80     10000

========================================
VGGNET - learning rate with custom decay
========================================
(dl4cv) (master) deep $ python cifar10_lr_decay.py -o vgg_cifar10_lr_decay.png -b yes
Using TensorFlow backend.
[INFO] loading CIFAR-10 data...
[INFO] compiling model...
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       
_________________________________________________________________
activation_1 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     
_________________________________________________________________
activation_4 (Activation)    (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 16, 16, 64)        256       
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 8, 8, 64)          0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 4096)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               2097664   
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 512)               2048      
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 10)                5130      
_________________________________________________________________
activation_6 (Activation)    (None, 10)                0         
=================================================================
Total params: 2,171,178
Trainable params: 2,169,770
Non-trainable params: 1,408
_________________________________________________________________
[INFO] training network...
Train on 50000 samples, validate on 10000 samples
Epoch 1/40
2017-11-21 17:29:23.435226: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 17:29:23.435519: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 17:29:23.435530: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-11-21 17:29:23.435536: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
50000/50000 [==============================] - 592s - loss: 1.6790 - acc: 0.4428 - val_loss: 1.4184 - val_acc: 0.5324
Epoch 2/40
50000/50000 [==============================] - 588s - loss: 1.2170 - acc: 0.5794 - val_loss: 1.0112 - val_acc: 0.6442
Epoch 3/40
50000/50000 [==============================] - 573s - loss: 1.0202 - acc: 0.6465 - val_loss: 1.0694 - val_acc: 0.6505
Epoch 4/40
50000/50000 [==============================] - 588s - loss: 0.9013 - acc: 0.6876 - val_loss: 0.8561 - val_acc: 0.7041
Epoch 5/40
50000/50000 [==============================] - 581s - loss: 0.8213 - acc: 0.7137 - val_loss: 0.7614 - val_acc: 0.7347
Epoch 6/40
50000/50000 [==============================] - 594s - loss: 0.7562 - acc: 0.7366 - val_loss: 0.8198 - val_acc: 0.7218
Epoch 7/40
50000/50000 [==============================] - 589s - loss: 0.7043 - acc: 0.7548 - val_loss: 0.6987 - val_acc: 0.7568
Epoch 8/40
50000/50000 [==============================] - 582s - loss: 0.6475 - acc: 0.7739 - val_loss: 0.6453 - val_acc: 0.7793
Epoch 9/40
50000/50000 [==============================] - 570s - loss: 0.6084 - acc: 0.7880 - val_loss: 0.6278 - val_acc: 0.7845
Epoch 10/40
50000/50000 [==============================] - 570s - loss: 0.5622 - acc: 0.8031 - val_loss: 0.6019 - val_acc: 0.8005
Epoch 11/40
50000/50000 [==============================] - 588s - loss: 0.5320 - acc: 0.8125 - val_loss: 0.6258 - val_acc: 0.7909
Epoch 12/40
50000/50000 [==============================] - 580s - loss: 0.5039 - acc: 0.8206 - val_loss: 0.6526 - val_acc: 0.7839
Epoch 13/40
50000/50000 [==============================] - 585s - loss: 0.4764 - acc: 0.8302 - val_loss: 0.5995 - val_acc: 0.8008
Epoch 14/40
50000/50000 [==============================] - 604s - loss: 0.4533 - acc: 0.8395 - val_loss: 0.6133 - val_acc: 0.7963
Epoch 15/40
50000/50000 [==============================] - 589s - loss: 0.4255 - acc: 0.8495 - val_loss: 0.5808 - val_acc: 0.8063
Epoch 16/40
50000/50000 [==============================] - 572s - loss: 0.4035 - acc: 0.8578 - val_loss: 0.5683 - val_acc: 0.8110
Epoch 17/40
50000/50000 [==============================] - 570s - loss: 0.3894 - acc: 0.8606 - val_loss: 0.6139 - val_acc: 0.8021
Epoch 18/40
50000/50000 [==============================] - 571s - loss: 0.3709 - acc: 0.8682 - val_loss: 0.5963 - val_acc: 0.8119
Epoch 19/40
50000/50000 [==============================] - 685s - loss: 0.3474 - acc: 0.8764 - val_loss: 0.5857 - val_acc: 0.8128
Epoch 20/40
50000/50000 [==============================] - 761s - loss: 0.3304 - acc: 0.8828 - val_loss: 0.5825 - val_acc: 0.8143
Epoch 21/40
50000/50000 [==============================] - 728s - loss: 0.3201 - acc: 0.8835 - val_loss: 0.5985 - val_acc: 0.8108
Epoch 22/40
50000/50000 [==============================] - 728s - loss: 0.3106 - acc: 0.8899 - val_loss: 0.5680 - val_acc: 0.8212
Epoch 23/40
50000/50000 [==============================] - 612s - loss: 0.2891 - acc: 0.8956 - val_loss: 0.6701 - val_acc: 0.7975
Epoch 24/40
50000/50000 [==============================] - 582s - loss: 0.2840 - acc: 0.8981 - val_loss: 0.6110 - val_acc: 0.8159
Epoch 25/40
50000/50000 [==============================] - 581s - loss: 0.2659 - acc: 0.9048 - val_loss: 0.5941 - val_acc: 0.8203
Epoch 26/40
50000/50000 [==============================] - 580s - loss: 0.2586 - acc: 0.9069 - val_loss: 0.5858 - val_acc: 0.8230
Epoch 27/40
50000/50000 [==============================] - 583s - loss: 0.2518 - acc: 0.9103 - val_loss: 0.5893 - val_acc: 0.8208
Epoch 28/40
50000/50000 [==============================] - 581s - loss: 0.2378 - acc: 0.9139 - val_loss: 0.6201 - val_acc: 0.8208
Epoch 29/40
50000/50000 [==============================] - 580s - loss: 0.2375 - acc: 0.9168 - val_loss: 0.5878 - val_acc: 0.8252
Epoch 30/40
50000/50000 [==============================] - 581s - loss: 0.2255 - acc: 0.9205 - val_loss: 0.5763 - val_acc: 0.8293
Epoch 31/40
50000/50000 [==============================] - 582s - loss: 0.2223 - acc: 0.9207 - val_loss: 0.5904 - val_acc: 0.8269
Epoch 32/40
50000/50000 [==============================] - 581s - loss: 0.2079 - acc: 0.9252 - val_loss: 0.6117 - val_acc: 0.8260
Epoch 33/40
50000/50000 [==============================] - 581s - loss: 0.2058 - acc: 0.9262 - val_loss: 0.6294 - val_acc: 0.8221
Epoch 34/40
50000/50000 [==============================] - 581s - loss: 0.1996 - acc: 0.9290 - val_loss: 0.6164 - val_acc: 0.8269
Epoch 35/40
50000/50000 [==============================] - 581s - loss: 0.2013 - acc: 0.9272 - val_loss: 0.6002 - val_acc: 0.8302
Epoch 36/40
50000/50000 [==============================] - 581s - loss: 0.1897 - acc: 0.9330 - val_loss: 0.5914 - val_acc: 0.8307
Epoch 37/40
50000/50000 [==============================] - 581s - loss: 0.1811 - acc: 0.9364 - val_loss: 0.6140 - val_acc: 0.8258
Epoch 38/40
50000/50000 [==============================] - 581s - loss: 0.1777 - acc: 0.9363 - val_loss: 0.6009 - val_acc: 0.8274
Epoch 39/40
50000/50000 [==============================] - 581s - loss: 0.1766 - acc: 0.9361 - val_loss: 0.6128 - val_acc: 0.8291
Epoch 40/40
50000/50000 [==============================] - 582s - loss: 0.1676 - acc: 0.9394 - val_loss: 0.6083 - val_acc: 0.8273
[INFO] evluating network...
             precision    recall  f1-score   support

   airplane       0.85      0.85      0.85      1000
 automobile       0.89      0.92      0.91      1000
       bird       0.81      0.71      0.76      1000
        cat       0.64      0.73      0.68      1000
       deer       0.80      0.79      0.80      1000
        dog       0.73      0.76      0.75      1000
       frog       0.91      0.84      0.87      1000
      horse       0.87      0.88      0.88      1000
       ship       0.91      0.89      0.90      1000
      truck       0.88      0.89      0.89      1000

avg / total       0.83      0.83      0.83     10000




